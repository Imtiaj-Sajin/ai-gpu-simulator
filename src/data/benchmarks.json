[
  {
    "id": "llama-3-1-8b-rtx4090-single-fp16-4k",
    "gpuId": "rtx-4090",
    "modelId": "llama-3-1-8b",
    "context": 4096,
    "precision": "fp16",
    "mode": "single",
    "batchSize": 1,
    "concurrency": 1,
    "prefillTps": 900,
    "decodeTps": 82,
    "source": { "label": "Reddit LocalLLaMA", "url": "https://www.reddit.com/r/LocalLLaMA/comments/1e6u031/llama3_8bs_performance_on_rtx_4090_gpu", "date": "2024-07-01" }
  },
  {
    "id": "llama-3-1-8b-rtx4090-single-int8-4k",
    "gpuId": "rtx-4090",
    "modelId": "llama-3-1-8b",
    "context": 4096,
    "precision": "int8",
    "mode": "single",
    "batchSize": 1,
    "concurrency": 1,
    "prefillTps": 950,
    "decodeTps": 87,
    "source": { "label": "Novita AI Medium", "url": "https://medium.com/@marketing_novita.ai/riding-the-lightning-llama3-8bs-performance-on-the-cutting-edge-rtx-4090-gpu-797153ee1762", "date": "2024-04-01" }
  },
  {
    "id": "llama-3-1-70b-h100-sxm-throughput-fp16-32k",
    "gpuId": "h100-sxm",
    "modelId": "llama-3-1-70b",
    "context": 32768,
    "precision": "fp16",
    "mode": "throughput",
    "batchSize": 32,
    "concurrency": 128,
    "prefillTps": 1200,
    "decodeTps": 250,
    "source": { "label": "Ori Blog", "url": "https://www.ori.co/blog/benchmarking-llama-3.1-8b-instruct-on-nvidia-h100-and-a100-chips-with-the-vllm-inferencing-engine", "date": "2024-10-11" }
  },
  {
    "id": "mixtral-8x7b-h100-sxm-single-fp16-32k",
    "gpuId": "h100-sxm",
    "modelId": "mixtral-8x7b",
    "context": 32768,
    "precision": "fp16",
    "mode": "single",
    "batchSize": 1,
    "concurrency": 1,
    "prefillTps": 573,
    "decodeTps": 65.8,
    "source": { "label": "NVIDIA Developer Blog", "url": "https://developer.nvidia.com/blog/achieving-high-mixtral-8x7b-performance-with-nvidia-h100-tensor-core-gpus-and-tensorrt-llm", "date": "2024-07-02" }
  },
  {
    "id": "qwen-2-5-72b-a100-80gb-throughput-bf16-6k",
    "gpuId": "a100-80gb",
    "modelId": "qwen-2-5-72b",
    "context": 6144,
    "precision": "fp16",
    "mode": "throughput",
    "batchSize": 2,
    "concurrency": 32,
    "prefillTps": 6.39,
    "decodeTps": 140,
    "source": { "label": "Qwen Docs", "url": "https://qwen.readthedocs.io/en/v2.5/benchmark/speed_benchmark.html", "date": "2024-09-01" }
  },
  {
    "id": "deepseek-r1-distill-32b-rtx4090-single-int8-4k",
    "gpuId": "rtx-4090",
    "modelId": "deepseek-r1-distill-32b",
    "context": 4096,
    "precision": "int8",
    "mode": "single",
    "batchSize": 1,
    "concurrency": 1,
    "prefillTps": 500,
    "decodeTps": 38,
    "source": { "label": "YouTube Benchmark", "url": "https://www.youtube.com/watch?v=FjhriU82FBs", "date": "2024-02-01" }
  },
  {
    "id": "mistral-7b-rtx-4070-ti-super-single-fp16-8k",
    "gpuId": "rtx-4070-ti-super",
    "modelId": "mistral-7b",
    "context": 8192,
    "precision": "fp16",
    "mode": "single",
    "batchSize": 1,
    "concurrency": 1,
    "prefillTps": 300,
    "decodeTps": 58.2,
    "source": { "label": "DEV Community", "url": "https://dev.to/maximsaplin/running-local-llms-cpu-vs-gpu-a-quick-speed-test-2cjn", "date": "2024-03-11" }
  },
  {
    "id": "rtx-pro-6000-blackwell-96gb-llama-3-1-70b-single-fp16-128k",
    "gpuId": "rtx-pro-6000-blackwell-96gb",
    "modelId": "llama-3-1-70b",
    "context": 128000,
    "precision": "fp16",
    "mode": "single",
    "batchSize": 1,
    "concurrency": 1,
    "prefillTps": 1200,
    "decodeTps": 31.8,
    "source": { "label": "StorageReview", "url": "https://www.storagereview.com/review/nvidia-rtx-pro-6000-workstation-gpu-review-blackwell-architecture-and-96-gb-for-pro-workflows", "date": "2025-10-07" }
  }
]