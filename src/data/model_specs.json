[
  {
    "id": "llama-3-1-8b",
    "name": "Llama 3.1 8B",
    "family": "llama",
    "paramsB": 8,
    "defaultContext": 128000,
    "notes": "Meta's multilingual LLM with long context.",
    "sources": [
      { "label": "Meta AI", "url": "https://ai.meta.com/blog/meta-llama-3-1", "date": "2024-07-23" }
    ]
  },
  {
    "id": "llama-3-1-70b",
    "name": "Llama 3.1 70B",
    "family": "llama",
    "paramsB": 70,
    "defaultContext": 128000,
    "notes": "High-performance LLM for reasoning.",
    "sources": [
      { "label": "Meta AI", "url": "https://ai.meta.com/blog/meta-llama-3-1", "date": "2024-07-23" },
      { "label": "Hugging Face", "url": "https://huggingface.co/meta-llama/Llama-3.1-70B-Instruct", "date": "2024-07-23" }
    ]
  },
  {
    "id": "mistral-7b",
    "name": "Mistral 7B",
    "family": "mistral",
    "paramsB": 7,
    "defaultContext": 8192,
    "notes": "Efficient MoE model.",
    "sources": [
      { "label": "Mistral AI", "url": "https://mistral.ai/news/announcing-mistral-7b/", "date": "2023-09-27" }
    ]
  },
  {
    "id": "mixtral-8x7b",
    "name": "Mixtral 8x7B",
    "family": "mistral",
    "paramsB": 46.7,
    "defaultContext": 32768,
    "notes": "MoE with 8 experts, active params ~12.9B.",
    "sources": [
      { "label": "Mistral AI", "url": "https://mistral.ai/news/mixtral-of-experts/", "date": "2023-12-08" },
      { "label": "NVIDIA Developer Blog", "url": "https://developer.nvidia.com/blog/achieving-high-mixtral-8x7b-performance-with-nvidia-h100-tensor-core-gpus-and-tensorrt-llm", "date": "2024-07-02" }
    ]
  },
  {
    "id": "mixtral-8x22b",
    "name": "Mixtral 8x22B",
    "family": "mistral",
    "paramsB": 141,
    "defaultContext": 65536,
    "notes": "Larger MoE for complex tasks.",
    "sources": [
      { "label": "Mistral AI", "url": "https://mistral.ai/news/mixtral-8x22b/", "date": "2024-04-10" }
    ]
  },
  {
    "id": "qwen-2-5-7b",
    "name": "Qwen2.5 7B",
    "family": "qwen",
    "paramsB": 7.61,
    "defaultContext": 131072,
    "notes": "Alibaba's efficient LLM.",
    "sources": [
      { "label": "Qwen Docs", "url": "https://qwen.readthedocs.io/en/v2.5/benchmark/speed_benchmark.html", "date": "2024-09-01" }
    ]
  },
  {
    "id": "qwen-2-5-72b",
    "name": "Qwen2.5 72B",
    "family": "qwen",
    "paramsB": 72,
    "defaultContext": 128000,
    "notes": "High-performance for reasoning, used in benchmarks on A100.",
    "sources": [
      { "label": "Qwen Docs", "url": "https://qwen.readthedocs.io/en/v2.5/benchmark/speed_benchmark.html", "date": "2024-09-01" },
      { "label": "SaladCloud Blog", "url": "https://blog.salad.com/llama-3-1-8b", "date": "2024-12-09" }
    ]
  },
  {
    "id": "deepseek-r1-distill-32b",
    "name": "DeepSeek R1 Distill 32B",
    "family": "deepseek",
    "paramsB": 32,
    "defaultContext": 32768,
    "notes": "Distilled for efficient inference, benchmarked on RTX 4090.",
    "sources": [
      { "label": "DeepSeek", "url": "https://github.com/DeepSeek-AI/DeepSeek-R1", "date": "2024-11-01" },
      { "label": "YouTube Benchmark", "url": "https://www.youtube.com/watch?v=FjhriU82FBs", "date": "2024-02-01" }
    ]
  }
]