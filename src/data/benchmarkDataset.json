{
  "gpus": [
    {
      "id": "rtx-4070-ti-super",
      "name": "RTX 4070 Ti Super",
      "tier": "consumer",
      "vramGB": 16,
      "memoryBandwidthGBs": 672.3,
      "fp16Tflops": 88.2,
      "architecture": "Ada Lovelace",
      "tdp": 285,
      "sources": [
        {
          "label": "TechPowerUp GPU Database",
          "url": "https://www.techpowerup.com/gpu-specs/geforce-rtx-4070-ti-super.c4187",
          "type": "spec_sheet"
        }
      ]
    },
    {
      "id": "rtx-4080-super",
      "name": "RTX 4080 Super",
      "tier": "consumer",
      "vramGB": 16,
      "memoryBandwidthGBs": 736.3,
      "fp16Tflops": 104.4,
      "architecture": "Ada Lovelace",
      "tdp": 320,
      "sources": [
        {
          "label": "TechPowerUp GPU Database",
          "url": "https://www.techpowerup.com/gpu-specs/geforce-rtx-4080-super.c4182",
          "type": "spec_sheet"
        }
      ]
    },
    {
      "id": "rtx-4090",
      "name": "RTX 4090",
      "tier": "consumer",
      "vramGB": 24,
      "memoryBandwidthGBs": 1008,
      "fp16Tflops": 165.2,
      "architecture": "Ada Lovelace",
      "tdp": 450,
      "cudaCores": 16384,
      "tensorCores": 512,
      "sources": [
        {
          "label": "TechPowerUp GPU Database",
          "url": "https://www.techpowerup.com/gpu-specs/geforce-rtx-4090.c3889",
          "type": "spec_sheet"
        },
        {
          "label": "NVIDIA Official",
          "url": "https://www.nvidia.com/en-us/geforce/graphics-cards/40-series/compare/",
          "type": "manufacturer"
        }
      ]
    },
    {
      "id": "rtx-5070-ti",
      "name": "RTX 5070 Ti",
      "tier": "consumer",
      "vramGB": 16,
      "memoryBandwidthGBs": 896,
      "fp16Tflops": 177.4,
      "architecture": "Blackwell 2.0",
      "tdp": 300,
      "cudaCores": 8960,
      "sources": [
        {
          "label": "TechPowerUp GPU Database",
          "url": "https://www.techpowerup.com/gpu-specs/geforce-rtx-5070-ti.c4243",
          "type": "spec_sheet"
        },
        {
          "label": "NVIDIA Official",
          "url": "https://www.nvidia.com/en-us/geforce/graphics-cards/50-series/compare/",
          "type": "manufacturer"
        }
      ]
    },
    {
      "id": "rtx-5080",
      "name": "RTX 5080",
      "tier": "consumer",
      "vramGB": 16,
      "memoryBandwidthGBs": 960,
      "fp16Tflops": 225.1,
      "architecture": "Blackwell 2.0",
      "tdp": 360,
      "cudaCores": 10752,
      "tensorCores": 336,
      "sources": [
        {
          "label": "TechPowerUp GPU Database",
          "url": "https://www.techpowerup.com/gpu-specs/geforce-rtx-5080.c4217",
          "type": "spec_sheet"
        },
        {
          "label": "NVIDIA Official",
          "url": "https://www.nvidia.com/en-us/geforce/graphics-cards/50-series/compare/",
          "type": "manufacturer"
        }
      ]
    },
    {
      "id": "rtx-5090",
      "name": "RTX 5090",
      "tier": "consumer",
      "vramGB": 32,
      "memoryBandwidthGBs": 1792,
      "fp16Tflops": 419.2,
      "architecture": "Blackwell 2.0",
      "tdp": 575,
      "cudaCores": 21760,
      "tensorCores": 680,
      "aiTops": 3352,
      "sources": [
        {
          "label": "TechPowerUp GPU Database",
          "url": "https://www.techpowerup.com/gpu-specs/geforce-rtx-5090.c4216",
          "type": "spec_sheet"
        },
        {
          "label": "NVIDIA Official",
          "url": "https://www.nvidia.com/en-us/geforce/graphics-cards/50-series/rtx-5090/",
          "type": "manufacturer"
        }
      ]
    },
    {
      "id": "rtx-a6000",
      "name": "RTX A6000",
      "tier": "workstation",
      "vramGB": 48,
      "memoryBandwidthGBs": 768,
      "fp16Tflops": 38.71,
      "architecture": "Ampere",
      "tdp": 300,
      "cudaCores": 10752,
      "sources": [
        {
          "label": "TechPowerUp GPU Database",
          "url": "https://www.techpowerup.com/gpu-specs/rtx-a6000.c3686",
          "type": "spec_sheet"
        }
      ]
    },
    {
      "id": "rtx-pro-6000-blackwell-96gb",
      "name": "RTX PRO 6000 Blackwell (96GB)",
      "tier": "workstation",
      "vramGB": 96,
      "memoryBandwidthGBs": 1792,
      "fp16Tflops": 126,
      "architecture": "Blackwell 2.0",
      "tdp": 600,
      "cudaCores": 24064,
      "tensorCores": 752,
      "aiTops": 4000,
      "sources": [
        {
          "label": "TechPowerUp GPU Database",
          "url": "https://www.techpowerup.com/gpu-specs/rtx-pro-6000-blackwell.c4272",
          "type": "spec_sheet"
        },
        {
          "label": "NVIDIA Official Datasheet",
          "url": "https://www.nvidia.com/content/dam/en-zz/Solutions/data-center/rtx-pro-6000-blackwell-workstation-edition/workstation-blackwell-rtx-pro-6000-workstation-edition-nvidia-us-3519208-web.pdf",
          "type": "manufacturer"
        }
      ]
    },
    {
      "id": "a100-80gb",
      "name": "A100 80GB",
      "tier": "datacenter",
      "vramGB": 80,
      "memoryBandwidthGBs": 2039,
      "fp16Tflops": 312,
      "tf32Tflops": 156,
      "architecture": "Ampere",
      "tdp": 300,
      "sources": [
        {
          "label": "NVIDIA Official",
          "url": "https://www.nvidia.com/en-us/data-center/a100/",
          "type": "manufacturer"
        },
        {
          "label": "TechPowerUp GPU Database",
          "url": "https://www.techpowerup.com/gpu-specs/a100-pcie-80-gb.c3821",
          "type": "spec_sheet"
        }
      ]
    },
    {
      "id": "h100-sxm",
      "name": "H100 (SXM)",
      "tier": "datacenter",
      "vramGB": 80,
      "memoryBandwidthGBs": 3350,
      "fp16Tflops": 1979,
      "fp8Tflops": 3958,
      "tf32Tflops": 989,
      "architecture": "Hopper",
      "tdp": 700,
      "sources": [
        {
          "label": "NVIDIA Official",
          "url": "https://www.nvidia.com/en-us/data-center/h100/",
          "type": "manufacturer"
        },
        {
          "label": "NVIDIA H100 Architecture Whitepaper",
          "url": "https://resources.nvidia.com/en-us-hopper-architecture/nvidia-h100-tensor-c",
          "type": "technical_doc"
        }
      ]
    },
    {
      "id": "h100-pcie",
      "name": "H100 (PCIe)",
      "tier": "datacenter",
      "vramGB": 80,
      "memoryBandwidthGBs": 2000,
      "fp16Tflops": 1513,
      "fp8Tflops": 3026,
      "tf32Tflops": 756,
      "architecture": "Hopper",
      "tdp": 350,
      "sources": [
        {
          "label": "NVIDIA Official",
          "url": "https://www.nvidia.com/en-us/data-center/h100/",
          "type": "manufacturer"
        }
      ]
    }
  ],
  "models": [
    {
      "id": "llama-3-1-8b",
      "name": "Llama 3.1 8B",
      "family": "llama",
      "paramsB": 8,
      "defaultContext": 128000,
      "modelSize": {
        "fp16GB": 16,
        "fp8GB": 8,
        "int8GB": 8,
        "int4GB": 4
      },
      "sources": [
        {
          "label": "Meta AI",
          "url": "https://ai.meta.com/blog/meta-llama-3-1/",
          "type": "official"
        }
      ]
    },
    {
      "id": "llama-3-1-70b",
      "name": "Llama 3.1 70B",
      "family": "llama",
      "paramsB": 70,
      "defaultContext": 128000,
      "modelSize": {
        "fp16GB": 140,
        "fp8GB": 70,
        "int8GB": 70,
        "int4GB": 35
      },
      "sources": [
        {
          "label": "Meta AI",
          "url": "https://ai.meta.com/blog/meta-llama-3-1/",
          "type": "official"
        }
      ]
    },
    {
      "id": "mistral-7b",
      "name": "Mistral 7B",
      "family": "mistral",
      "paramsB": 7,
      "defaultContext": 8192,
      "modelSize": {
        "fp16GB": 14,
        "int4GB": 4
      },
      "sources": [
        {
          "label": "Mistral AI",
          "url": "https://mistral.ai/news/announcing-mistral-7b/",
          "type": "official"
        }
      ]
    },
    {
      "id": "mixtral-8x7b",
      "name": "Mixtral 8x7B",
      "family": "mistral",
      "paramsB": 46.7,
      "defaultContext": 32768,
      "modelSize": {
        "fp16GB": 93,
        "int4GB": 26
      },
      "sources": [
        {
          "label": "Mistral AI",
          "url": "https://mistral.ai/news/mixtral-of-experts/",
          "type": "official"
        }
      ]
    },
    {
      "id": "mixtral-8x22b",
      "name": "Mixtral 8x22B",
      "family": "mistral",
      "paramsB": 141,
      "defaultContext": 65536,
      "modelSize": {
        "fp16GB": 282,
        "int4GB": 80
      },
      "sources": [
        {
          "label": "Mistral AI",
          "url": "https://mistral.ai/news/mixtral-8x22b/",
          "type": "official"
        }
      ]
    },
    {
      "id": "qwen-2-5-7b",
      "name": "Qwen2.5 7B",
      "family": "qwen",
      "paramsB": 7.61,
      "defaultContext": 131072,
      "modelSize": {
        "fp16GB": 15,
        "int4GB": 4.5
      },
      "sources": [
        {
          "label": "Qwen Team",
          "url": "https://qwenlm.github.io/blog/qwen2.5/",
          "type": "official"
        }
      ]
    },
    {
      "id": "qwen-2-5-14b",
      "name": "Qwen2.5 14B",
      "family": "qwen",
      "paramsB": 14.7,
      "defaultContext": 131072,
      "modelSize": {
        "fp16GB": 29,
        "int4GB": 9
      },
      "sources": [
        {
          "label": "Qwen Team",
          "url": "https://qwenlm.github.io/blog/qwen2.5/",
          "type": "official"
        }
      ]
    },
    {
      "id": "qwen-2-5-32b",
      "name": "Qwen2.5 32B",
      "family": "qwen",
      "paramsB": 32.5,
      "defaultContext": 131072,
      "modelSize": {
        "fp16GB": 65,
        "int4GB": 20
      },
      "sources": [
        {
          "label": "Qwen Team",
          "url": "https://qwenlm.github.io/blog/qwen2.5/",
          "type": "official"
        }
      ]
    },
    {
      "id": "qwen-2-5-72b",
      "name": "Qwen2.5 72B",
      "family": "qwen",
      "paramsB": 72,
      "defaultContext": 128000,
      "modelSize": {
        "fp16GB": 144,
        "int4GB": 41
      },
      "sources": [
        {
          "label": "Qwen Team",
          "url": "https://qwenlm.github.io/blog/qwen2.5/",
          "type": "official"
        }
      ]
    },
    {
      "id": "deepseek-r1-distill-32b",
      "name": "DeepSeek R1 Distill 32B",
      "family": "deepseek",
      "paramsB": 32,
      "defaultContext": 32768,
      "modelSize": {
        "fp16GB": 64,
        "int4GB": 19
      },
      "sources": [
        {
          "label": "DeepSeek AI",
          "url": "https://github.com/deepseek-ai/DeepSeek-R1",
          "type": "official"
        }
      ]
    }
  ],
  "benchmarks": [
    {
      "id": "bench-rtx4090-llama8b-fp16-single",
      "gpuId": "rtx-4090",
      "modelId": "llama-3-1-8b",
      "context": 4096,
      "precision": "fp16",
      "mode": "single",
      "batchSize": 1,
      "concurrency": 1,
      "prefillTps": 900,
      "decodeTps": 110,
      "framework": "vLLM",
      "sources": [
        {
          "label": "ermolushka vLLM Benchmark",
          "url": "https://ermolushka.github.io/posts/vllm-benchmark-4090/",
          "date": "2024-09",
          "note": "FP16 baseline: 110 tok/s decode"
        },
        {
          "label": "Ollama RTX 4090 Benchmark",
          "url": "https://www.databasemart.com/blog/ollama-gpu-benchmark-rtx4090",
          "note": "Evaluation speeds up to 70 tokens/s for LLaMA models"
        }
      ]
    },
    {
      "id": "bench-rtx4090-llama8b-awq-single",
      "gpuId": "rtx-4090",
      "modelId": "llama-3-1-8b",
      "context": 4096,
      "precision": "int8",
      "mode": "single",
      "batchSize": 1,
      "concurrency": 1,
      "prefillTps": 4500,
      "decodeTps": 579,
      "framework": "vLLM (AWQ)",
      "sources": [
        {
          "label": "ermolushka vLLM Benchmark",
          "url": "https://ermolushka.github.io/posts/vllm-benchmark-4090/",
          "date": "2024-09",
          "note": "AWQ achieves 579 tokens/second"
        }
      ]
    },
    {
      "id": "bench-rtx4090-llama8b-gptq-single",
      "gpuId": "rtx-4090",
      "modelId": "llama-3-1-8b",
      "context": 4096,
      "precision": "int8",
      "mode": "single",
      "batchSize": 1,
      "concurrency": 1,
      "prefillTps": 4650,
      "decodeTps": 599,
      "framework": "vLLM (GPTQ)",
      "sources": [
        {
          "label": "ermolushka vLLM Benchmark",
          "url": "https://ermolushka.github.io/posts/vllm-benchmark-4090/",
          "date": "2024-09",
          "note": "GPTQ achieves 598.7 tokens/second"
        }
      ]
    },
    {
      "id": "bench-rtx5090-qwen7b-throughput",
      "gpuId": "rtx-5090",
      "modelId": "qwen-2-5-7b",
      "context": 4096,
      "precision": "fp16",
      "mode": "single",
      "batchSize": 1,
      "concurrency": 1,
      "prefillTps": 7000,
      "decodeTps": 5841,
      "framework": "vLLM",
      "sources": [
        {
          "label": "Introl Local LLM Hardware Guide 2025",
          "url": "https://introl.com/blog/local-llm-hardware-pricing-guide-2025",
          "date": "2025-08",
          "note": "RTX 5090 achieves 5,841 tokens/second on Qwen2.5-Coder-7B"
        }
      ]
    },
    {
      "id": "bench-a100-80gb-llama8b-fp16-single",
      "gpuId": "a100-80gb",
      "modelId": "llama-3-1-8b",
      "context": 2048,
      "precision": "fp16",
      "mode": "single",
      "batchSize": 1,
      "concurrency": 1,
      "prefillTps": 2100,
      "decodeTps": 120,
      "framework": "vLLM",
      "sources": [
        {
          "label": "Ori H100 vs A100 Benchmark",
          "url": "https://www.ori.co/blog/benchmarking-llama-3.1-8b-instruct-on-nvidia-h100-and-a100-chips-with-the-vllm-inferencing-engine",
          "note": "A100 throughput benchmarks with vLLM"
        }
      ]
    },
    {
      "id": "bench-a100-80gb-llama70b-fp16-single",
      "gpuId": "a100-80gb",
      "modelId": "llama-3-1-70b",
      "context": 4096,
      "precision": "fp16",
      "mode": "single",
      "batchSize": 1,
      "concurrency": 1,
      "prefillTps": 420,
      "decodeTps": 35,
      "framework": "vLLM",
      "sources": [
        {
          "label": "Introl Local LLM Hardware Guide 2025",
          "url": "https://introl.com/blog/local-llm-hardware-pricing-guide-2025",
          "date": "2025-08",
          "note": "Llama 3.1 70B with AWQ 4-bit quantization on dual A100 80GB GPUs delivers enterprise-grade performance at 35GB per GPU utilization"
        }
      ]
    },
    {
      "id": "bench-h100-sxm-llama8b-fp16-single",
      "gpuId": "h100-sxm",
      "modelId": "llama-3-1-8b",
      "context": 2048,
      "precision": "fp16",
      "mode": "single",
      "batchSize": 1,
      "concurrency": 1,
      "prefillTps": 8000,
      "decodeTps": 250,
      "framework": "vLLM",
      "sources": [
        {
          "label": "Ori H100 vs A100 Benchmark",
          "url": "https://www.ori.co/blog/benchmarking-llama-3.1-8b-instruct-on-nvidia-h100-and-a100-chips-with-the-vllm-inferencing-engine",
          "note": "H100 throughput benchmarks with vLLM"
        }
      ]
    },
    {
      "id": "bench-h100-sxm-llama8b-throughput",
      "gpuId": "h100-sxm",
      "modelId": "llama-3-1-8b",
      "context": 4096,
      "precision": "fp16",
      "mode": "throughput",
      "batchSize": 16,
      "concurrency": 16,
      "prefillTps": 12000,
      "decodeTps": 2400,
      "framework": "vLLM 0.6.0",
      "sources": [
        {
          "label": "vLLM v0.6.0 Performance Update",
          "url": "https://blog.vllm.ai/2024/09/05/perf-update.html",
          "date": "2024-09-05",
          "note": "vLLM 0.6.0 delivers 2,300-2,500 tokens/second for Llama 8B on H100"
        },
        {
          "label": "Introl Local LLM Hardware Guide",
          "url": "https://introl.com/blog/local-llm-hardware-pricing-guide-2025",
          "date": "2025-08",
          "note": "vLLM 0.6.0 delivers 2.7x throughput improvement achieving 2,300-2,500 tokens/second for Llama 8B on H100"
        }
      ]
    },
    {
      "id": "bench-h100-sxm-llama8b-sglang",
      "gpuId": "h100-sxm",
      "modelId": "llama-3-1-8b",
      "context": 4096,
      "precision": "fp16",
      "mode": "throughput",
      "batchSize": 64,
      "concurrency": 64,
      "prefillTps": 15000,
      "decodeTps": 16200,
      "framework": "SGLang",
      "sources": [
        {
          "label": "LLM Inference Engines Comparison",
          "url": "https://research.aimultiple.com/inference-engines/",
          "note": "SGLang achieves ~16,200 tok/s on H100 for Llama 3.1 8B"
        }
      ]
    },
    {
      "id": "bench-h100-sxm-llama70b-fp16-tensor-parallel",
      "gpuId": "h100-sxm",
      "modelId": "llama-3-1-70b",
      "context": 4096,
      "precision": "fp16",
      "mode": "throughput",
      "batchSize": 8,
      "concurrency": 8,
      "prefillTps": 3500,
      "decodeTps": 180,
      "framework": "vLLM (4x H100)",
      "sources": [
        {
          "label": "Hyperstack H100 vs A100 Benchmark",
          "url": "https://www.hyperstack.cloud/technical-resources/performance-benchmarks/llm-inference-benchmark-comparing-nvidia-a100-nvlink-vs-nvidia-h100-sxm",
          "date": "2025-09",
          "note": "4x H100 SXM5 benchmark for Llama 3.1 70B"
        }
      ]
    },
    {
      "id": "bench-h100-sxm-llama70b-fp8-tensorrt",
      "gpuId": "h100-sxm",
      "modelId": "llama-3-1-70b",
      "context": 2048,
      "precision": "int8",
      "mode": "throughput",
      "batchSize": 14,
      "concurrency": 8,
      "prefillTps": 4200,
      "decodeTps": 280,
      "framework": "TensorRT-LLM (8x H100 FP8)",
      "sources": [
        {
          "label": "NVIDIA TensorRT-LLM H100 Performance",
          "url": "https://developer.nvidia.com/blog/achieving-top-inference-performance-with-the-nvidia-h100-tensor-core-gpu-and-nvidia-tensorrt-llm/",
          "date": "2023-12-14",
          "note": "8-GPU DGX H100 can process over five Llama 2 70B inferences per second with 2,048 input and 128 output tokens"
        }
      ]
    }
  ],
  "metadata": {
    "version": "1.0.0",
    "lastUpdated": "2025-01-29",
    "description": "Comprehensive GPU and LLM benchmark dataset with verified sources",
    "notes": [
      "All benchmarks use real-world data from published sources",
      "Throughput values represent tokens per second",
      "Context lengths vary based on use case and dataset",
      "Framework choice significantly impacts performance",
      "Quantization (INT8, FP8, AWQ, GPTQ) provides substantial speedup"
    ]
  }
}