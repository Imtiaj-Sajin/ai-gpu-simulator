<!-- G:\codes\ai-gpu-simulator\index.html -->
<!doctype html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />

  <title>LLM GPU Performance Simulator | Test AI Model Speed, VRAM, Tokens/sec</title>

  <meta name="description" content="Simulate LLM/AI model performance on GPUs before buying hardware. Estimate VRAM usage, tokens per second, time to first token, and inference speed for models like Llama, Qwen, Mixtral and more." />

  <meta name="author" content="Imtiaj Sajin" />
  <link rel="canonical" href="https://aigpusim.vercel.app/" />

  <!-- Open Graph -->
  <meta property="og:title" content="LLM GPU Performance Simulator | Estimate AI Model Speed & VRAM" />
  <meta property="og:description" content="Test how fast LLM models run on different GPUs. Compare VRAM requirements, decode speed, and inference performance before investing in hardware." />
  <meta property="og:type" content="website" />
  <meta property="og:url" content="https://aigpusim.vercel.app/" />
  <meta property="og:image" content="https://aigpusim.vercel.app/landing/hompage preview.jpeg" />

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image" />
  <meta name="twitter:title" content="LLM GPU Performance Simulator" />
  <meta name="twitter:description" content="Estimate AI model speed on GPUs. VRAM, tokens/sec, latency, inference performance." />
  <meta name="twitter:image" content="https://aigpusim.vercel.app/landing/hompage preview.jpeg" />

  <!-- Google Site Verification -->
  <meta name="google-site-verification" content="GbWEcI9tjDTm_oT6U5tZB0dUCUzZp6fr-vLjEDNAvTc" />

  <!-- SEO Keywords (not very powerful anymore but still useful) -->
  <meta name="keywords" content="
    LLM GPU performance,
    AI GPU simulator,
    LLM inference speed,
    tokens per second GPU,
    VRAM calculator LLM,
    GPU for Llama,
    GPU for Qwen,
    AI hardware estimator,
    LLM benchmark tool,
    inference performance simulator,
    Test your GPU for AI models,
    LLM speed test,
    AI model VRAM usage,
    GPU comparison for LLMs,
    LLM latency estimator,
    AI GPU performance predictor,
    LLM hardware requirements,
    Dont blind buy GPU for AI,
    LLM decode speed test,
  " />

  <!-- Robots -->
  <meta name="robots" content="index, follow" />
</head>


  <body>
    <div id="root"></div>
    <script type="module" src="/src/main.tsx"></script>
  </body>
</html>
